{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw = './data/'\n",
    "path_out='./outputs/'\n",
    "path_out_images='./images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 LOADING DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(os.path.join(path_raw,'FLXDC_2023 (TCT)_20240612.csv'), low_memory=False, sep=';')\n",
    "cod_base_guru = pd.read_csv(os.path.join(path_raw,'cods_guru_plantas.csv'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_float(val_str):    \n",
    "    return float(val_str.replace('.','').replace(\",\",'.'))\n",
    "\n",
    "def class_to_num(df, col):\n",
    "    new_class = {name:num for num,name in enumerate(df[col].unique())}\n",
    "    return new_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geral = data_raw.copy()\n",
    "\n",
    "df_geral['Peso Liquido Delivery'] = df_geral['Peso Liquido Delivery'].map(lambda x: str_to_float(x) if x != '' else 0)\n",
    "df_geral['FRETE'] = df_geral['FRETE'].map(lambda x: str_to_float(x[1:]) if x != '' else 0)\n",
    "df_geral['FRETE AJUSTADO'] = df_geral['FRETE AJUSTADO'].map(lambda x: float(x.replace(',','')))\n",
    "df_geral['KM Ajustado'] = df_geral['KM Ajustado'].map(lambda x: float(x.replace(',','')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geral = df_geral[~df_geral['UF Destino'].isin(['0','1'])]\n",
    "df_geral = df_geral[df_geral['FRETE']>= 0.1]\n",
    "df_geral = df_geral[df_geral['Planta'] != '7958']\n",
    "df_geral.loc[\n",
    "    (df_geral['Planta'] == '7961') \n",
    "    & (df_geral['CHAVE ORIGEM'] == 'CONTAGEMMG'), \n",
    "    'CHAVE ORIGEM'\n",
    "] = 'SETE LAGOASMG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Agreggation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= [\n",
    "    'Planta', 'CHAVE ORIGEM','Movimentação',\n",
    "    'UF Destino', 'TIPO',\n",
    "]\n",
    "\n",
    "num_cols = [\n",
    "    'Quantidade','FRETE AJUSTADO',\n",
    "]\n",
    "\n",
    "df_agg = df_geral[\n",
    "    ['CHAVE DESTINO']+ features + num_cols\n",
    "    ].groupby(\n",
    "        ['CHAVE DESTINO']+features).sum().reset_index()\n",
    "\n",
    "_df_agg_mean = df_geral[\n",
    "    ['Nro.Documento','Data','CHAVE DESTINO']+ features + num_cols\n",
    "    ].groupby(\n",
    "        ['Nro.Documento','Data','CHAVE DESTINO']+features).sum().reset_index()\n",
    "_df_agg_mean['RealCaixa'] = _df_agg_mean['FRETE AJUSTADO'] / _df_agg_mean['Quantidade']\n",
    "\n",
    "_df_agg_mean = _df_agg_mean[\n",
    "    ['CHAVE DESTINO']+ features + num_cols + ['RealCaixa']\n",
    "    ].groupby(\n",
    "        ['CHAVE DESTINO']+features).mean().reset_index()\n",
    "\n",
    "df_agg = pd.merge(\n",
    "    df_agg,\n",
    "    _df_agg_mean[['CHAVE DESTINO']+ features + ['RealCaixa']].rename(columns={'RealCaixa':'RealCaixaMean'}),\n",
    "    on=['CHAVE DESTINO']+ features,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "_df_agg_km = df_geral[['CHAVE ORIGEM','CHAVE DESTINO','KM']].drop_duplicates()\n",
    "\n",
    "df_agg = pd.merge(\n",
    "    df_agg,\n",
    "    _df_agg_km.rename(columns={'KM':'KM Ajustado'}),\n",
    "    on=['CHAVE ORIGEM','CHAVE DESTINO'],\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_target = 'RealCaixaMean'\n",
    "\n",
    "### Initial hard filter\n",
    "df_agg = df_agg[df_agg[col_target] < df_agg[col_target].mean() * 10]\n",
    "\n",
    "### Categorical to Numerical\n",
    "for feature in features:\n",
    "    new_class = class_to_num(df_agg, feature)\n",
    "    df_agg[feature+'_Class'] = df_agg[feature].map(lambda x: new_class[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Filter to get the data in the middle 50% \n",
    "# df_qt = df_agg[\n",
    "#     features + ['CHAVE DESTINO'] + [col_target]\n",
    "#     ].groupby(\n",
    "#        features + ['CHAVE DESTINO']\n",
    "#     ).quantile(q=.25).reset_index().rename(columns={col_target:'%25'})\n",
    "# df_qt['%75'] = df_agg[\n",
    "#     features + ['CHAVE DESTINO'] + [col_target]\n",
    "#     ].groupby(\n",
    "#        features + ['CHAVE DESTINO']\n",
    "#     ).quantile(q=.75).reset_index()[col_target]\n",
    "\n",
    "# df_qt['Len'] = df_agg[\n",
    "#     features + ['CHAVE DESTINO'] + [col_target]\n",
    "#     ].groupby(\n",
    "#        features + ['CHAVE DESTINO']\n",
    "#     ).count().reset_index()[col_target]\n",
    "\n",
    "\n",
    "# df_agg = pd.merge(\n",
    "#     df_agg,\n",
    "#     df_qt,\n",
    "#     on=features + ['CHAVE DESTINO'],\n",
    "#     how='left',\n",
    "# )\n",
    "\n",
    "# df_filt_len2 = df_agg[df_agg['Len'] <3].copy()\n",
    "# df_filt = df_agg[(df_agg[col_target]>= df_agg['%25']) & (df_agg[col_target]<=df_agg['%75'])].copy()\n",
    "# df_filt = pd.concat(\n",
    "#     [\n",
    "#         df_filt,\n",
    "#         df_filt_len2\n",
    "#     ],\n",
    "#     ignore_index=True\n",
    "# )\n",
    "\n",
    "# df_filt['VENDAS'] = df_filt['Movimentação'] == 'VENDAS'\n",
    "# df_filt['TRANSFERENCIA'] = df_filt['Movimentação'] == 'TRANSFERENCIA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_classes = [f\"{feature}_Class\" for feature in features if feature != 'CHAVE ORIGEM']\n",
    "# params = {\n",
    "#     \"n_estimators\": 500,\n",
    "#     \"max_depth\": 4,\n",
    "#     \"min_samples_split\": 5,\n",
    "#     \"learning_rate\": 0.01,\n",
    "#     \"loss\": \"squared_error\",\n",
    "# }\n",
    "\n",
    "# X_Model = df_filt[['KM Ajustado','VENDAS','TRANSFERENCIA',]+eng_classes]\n",
    "# y_target = df_filt[col_target].to_numpy()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_Model, y_target, test_size=0.10, random_state=7, shuffle = True)\n",
    "\n",
    "# rf = RandomForestRegressor(random_state=2)\n",
    "# rf = rf.fit(X = X_Model, y = y_target)\n",
    "# y_pred_rf = rf.predict(X_Model)\n",
    "# r2_rf_test = r2_score(y_target, y_pred_rf)\n",
    "# mape_rf_test = mean_absolute_percentage_error(y_target, y_pred_rf)\n",
    "\n",
    "# fig,ax = plt.subplots(layout='constrained')\n",
    "# ax.plot(df_filt['KM Ajustado'],df_filt[col_target],'.')\n",
    "# ax.plot(df_filt['KM Ajustado'], rf.predict(X_Model),'.')\n",
    "# ax.text(df_filt['KM Ajustado'].max()-500,max(y_pred_rf),f'r2={r2_rf_test:0.2}')\n",
    "# ax.text(df_filt['KM Ajustado'].max()-500,max(y_pred_rf)-10,f'mape={mape_rf_test*100:.1f}')\n",
    "# fig.legend(['Dado','Modelo'],loc='upper right')\n",
    "# ax.set_xlabel('Km')\n",
    "# ax.set_ylabel('R$/CX')\n",
    "# ax.set_title('Modelo Geral - Random Forest')\n",
    "# os.makedirs(path_out_images, exist_ok=True)\n",
    "# os.makedirs(path_out, exist_ok=True)\n",
    "# fig.savefig(os.path.join(path_out_images,f'Modelo_Geral_RF.png'),dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 EXPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportação concluída!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando treinamento com ajuste fino...\n",
      "Preparando features...\n",
      "Criando estratificação...\n",
      "Dividindo dados...\n",
      "Treinando modelo - Etapa 1 (Grid Inicial)...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 408\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# 8. Execução\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# print(\"Iniciando treinamento...\")\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# model, X_train, X_test, y_train, y_test = train_ensemble_model(X_Model, y_target, df_filt)\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# metrics, fig = evaluate_model(model, X_train, X_test, y_train, y_test)\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIniciando treinamento com ajuste fino...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 408\u001b[0m model, X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ensemble_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_Model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_filt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m metrics, fig \u001b[38;5;241m=\u001b[39m evaluate_model(model, X_train, X_test, y_train, y_test)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# 9. Salvamento e resultados\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 279\u001b[0m, in \u001b[0;36mtrain_ensemble_model\u001b[0;34m(X, y, df_original)\u001b[0m\n\u001b[1;32m    269\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m create_ensemble_pipeline()\n\u001b[1;32m    270\u001b[0m grid_search_1 \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m    271\u001b[0m     pipeline,\n\u001b[1;32m    272\u001b[0m     param_grid_inicial,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    277\u001b[0m )\n\u001b[0;32m--> 279\u001b[0m \u001b[43mgrid_search_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensemble__sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Extrair melhores parâmetros\u001b[39;00m\n\u001b[1;32m    282\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search_1\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1023\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1018\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1023\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1570\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1569\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1570\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:969\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    965\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    966\u001b[0m         )\n\u001b[1;32m    967\u001b[0m     )\n\u001b[0;32m--> 969\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 0. Preparação dos dados iniciais\n",
    "# Definir features categóricas\n",
    "features = [\n",
    "    'Planta', 'CHAVE ORIGEM', 'Movimentação',\n",
    "    'UF Destino', 'TIPO',\n",
    "]\n",
    "\n",
    "# Criar eng_classes (colunas categóricas transformadas)\n",
    "eng_classes = [f\"{feature}_Class\" for feature in features if feature != 'CHAVE ORIGEM']\n",
    "\n",
    "# Preparar X e y inicial\n",
    "X_Model = df_filt[['KM Ajustado','VENDAS','TRANSFERENCIA'] + eng_classes]\n",
    "y_target = df_filt[col_target].to_numpy()\n",
    "\n",
    "# 1. Imports necessários\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import (\n",
    "    VotingRegressor, \n",
    "    GradientBoostingRegressor, \n",
    "    ExtraTreesRegressor,\n",
    "    RandomForestRegressor\n",
    ")\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# 2. Funções auxiliares\n",
    "def remove_outliers(X, y, z_threshold=2.5):\n",
    "    \"\"\"Remove outliers usando z-score\"\"\"\n",
    "    scaler = RobustScaler()\n",
    "    z_scores = np.abs(scaler.fit_transform(y.reshape(-1, 1)))\n",
    "    mask = z_scores < z_threshold\n",
    "    return X[mask.flatten()], y[mask.flatten()]\n",
    "\n",
    "def post_process_predictions(y_pred, y_train):\n",
    "    \"\"\"Pós-processa as previsões\"\"\"\n",
    "    y_pred = np.maximum(y_pred, 0)\n",
    "    upper_limit = y_train.mean() + 3 * y_train.std()\n",
    "    y_pred = np.minimum(y_pred, upper_limit)\n",
    "    return y_pred\n",
    "\n",
    "def compute_sample_weights(y):\n",
    "    \"\"\"Calcula pesos das amostras\"\"\"\n",
    "    bins = pd.qcut(y, q=10, labels=False)\n",
    "    weights = 1 / pd.Series(bins).value_counts()[bins].values\n",
    "    return weights / weights.sum() * len(weights)\n",
    "\n",
    "# 3. Feature Engineering\n",
    "def create_features(X, df_original):\n",
    "    \"\"\"Cria features engineered com foco nas interações mais importantes\"\"\"\n",
    "    X_new = X.copy()\n",
    "    \n",
    "    # Features básicas de KM\n",
    "    X_new['km_log'] = np.log1p(X['KM Ajustado'])\n",
    "    X_new['km_root'] = np.sqrt(X['KM Ajustado'])\n",
    "    X_new['km_squared'] = X['KM Ajustado'] ** 2\n",
    "    X_new['km_cubic'] = X['KM Ajustado'] ** 3\n",
    "    \n",
    "    # Interações com KM\n",
    "    X_new['km_vendas'] = X['KM Ajustado'] * X['VENDAS']\n",
    "    X_new['km_transfer'] = X['KM Ajustado'] * X['TRANSFERENCIA']\n",
    "    \n",
    "    # Interações com features importantes\n",
    "    X_new['planta_km'] = X['KM Ajustado'] * X['Planta_Class']\n",
    "    X_new['uf_km'] = X['KM Ajustado'] * X['UF Destino_Class']\n",
    "    X_new['tipo_km'] = X['KM Ajustado'] * X['TIPO_Class']\n",
    "    \n",
    "    # Features de agrupamento básicas\n",
    "    for feature in ['Planta', 'UF Destino', 'TIPO']:\n",
    "        X_new[f'mean_by_{feature}'] = df_original.groupby(feature)[col_target].transform('mean')\n",
    "        X_new[f'std_by_{feature}'] = df_original.groupby(feature)[col_target].transform('std')\n",
    "        X_new[f'median_by_{feature}'] = df_original.groupby(feature)[col_target].transform('median')\n",
    "    \n",
    "    # Novas interações baseadas nas features mais importantes\n",
    "    X_new['km_vendas_planta'] = X_new['km_vendas'] * X_new['mean_by_Planta']\n",
    "    X_new['km_vendas_tipo'] = X_new['km_vendas'] * X_new['mean_by_TIPO']\n",
    "    \n",
    "    # Features de densidade/frequência\n",
    "    X_new['densidade_regiao'] = df_original.groupby('UF Destino')['CHAVE DESTINO'].transform('count')\n",
    "    X_new['freq_planta'] = df_original.groupby('Planta')['CHAVE DESTINO'].transform('count')\n",
    "    \n",
    "    # Interações complexas\n",
    "    X_new['planta_tipo_mean'] = df_original.groupby(['Planta', 'TIPO'])[col_target].transform('mean')\n",
    "    X_new['uf_tipo_mean'] = df_original.groupby(['UF Destino', 'TIPO'])[col_target].transform('mean')\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "# 4. Pipeline e Grid\n",
    "# def create_ensemble_pipeline():\n",
    "#     \"\"\"Cria pipeline com ensemble de modelos\"\"\"\n",
    "#     rf = RandomForestRegressor(\n",
    "#         n_estimators=500,\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1,\n",
    "#         warm_start=True,\n",
    "#         max_features='sqrt'\n",
    "#     )\n",
    "    \n",
    "#     gb = GradientBoostingRegressor(\n",
    "#         n_estimators=500,\n",
    "#         random_state=42,\n",
    "#         learning_rate=0.01,\n",
    "#         warm_start=True,\n",
    "#         subsample=0.8\n",
    "#     )\n",
    "    \n",
    "#     et = ExtraTreesRegressor(\n",
    "#         n_estimators=500,\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1,\n",
    "#         warm_start=True,\n",
    "#         max_features='sqrt'\n",
    "#     )\n",
    "    \n",
    "#     hub = HuberRegressor(\n",
    "#         max_iter=2000,\n",
    "#         epsilon=1.35,\n",
    "#         tol=1e-3,\n",
    "#         warm_start=True,\n",
    "#         fit_intercept=True\n",
    "#     )\n",
    "    \n",
    "#     ensemble = VotingRegressor([\n",
    "#         ('rf', rf),\n",
    "#         ('gb', gb),\n",
    "#         ('et', et),\n",
    "#         ('hub', hub)\n",
    "#     ], weights=[0.4, 0.3, 0.2, 0.1])\n",
    "    \n",
    "#     return Pipeline([\n",
    "#         ('scaler', RobustScaler()),\n",
    "#         ('ensemble', ensemble)\n",
    "#     ])\n",
    "\n",
    "# 4. Pipeline \n",
    "def create_ensemble_pipeline():\n",
    "    \"\"\"Cria pipeline com ensemble de modelos otimizado\"\"\"\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        warm_start=True,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        oob_score=True  \n",
    "    )\n",
    "    \n",
    "    gb = GradientBoostingRegressor(\n",
    "        n_estimators=1000,\n",
    "        random_state=42,\n",
    "        warm_start=True,\n",
    "        validation_fraction=0.15,\n",
    "        n_iter_no_change=20,     \n",
    "        tol=1e-5,\n",
    "        subsample=0.8\n",
    "    )\n",
    "    \n",
    "    et = ExtraTreesRegressor(\n",
    "        n_estimators=500,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        warm_start=True,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True\n",
    "    )\n",
    "    \n",
    "    hub = HuberRegressor(\n",
    "        max_iter=5000,        \n",
    "        epsilon=1.35,\n",
    "        tol=1e-4,            \n",
    "        warm_start=True,\n",
    "        fit_intercept=True,\n",
    "        alpha=0.001         \n",
    "    )\n",
    "    \n",
    "  \n",
    "    ensemble = VotingRegressor([\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('et', et),\n",
    "        ('hub', hub)\n",
    "    ], weights=[0.50, 0.35, 0.10, 0.05])\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('ensemble', ensemble)\n",
    "    ])\n",
    "\n",
    "# 5. Grid de hiperparâmetros\n",
    "# param_grid = {\n",
    "#     'ensemble__rf__max_depth': [8, 10],\n",
    "#     'ensemble__rf__min_samples_split': [2, 5],\n",
    "#     'ensemble__gb__learning_rate': [0.01, 0.05],\n",
    "#     'ensemble__gb__subsample': [0.8, 0.9]\n",
    "# }\n",
    "\n",
    "param_grid_inicial = {\n",
    "    'ensemble__rf__max_depth': [6, 8, 10],\n",
    "    'ensemble__rf__min_samples_split': [2, 5, 10],\n",
    "    'ensemble__gb__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'ensemble__gb__subsample': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# 6. Treinamento\n",
    "# def train_ensemble_model(X, y, df_original):\n",
    "#     \"\"\"Treina o modelo ensemble\"\"\"\n",
    "#     print(\"Preparando features...\")\n",
    "#     X_processed = create_features(X, df_original)\n",
    "#     X_clean, y_clean = remove_outliers(X_processed, y, z_threshold=2.5)\n",
    "    \n",
    "#     y_log = np.log1p(y_clean)\n",
    "#     print(\"Criando estratificação...\")\n",
    "#     y_bins = pd.qcut(y_log, q=10, labels=False)\n",
    "    \n",
    "#     print(\"Dividindo dados...\")\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X_clean, y_log,\n",
    "#         test_size=0.2,\n",
    "#         random_state=42,\n",
    "#         stratify=y_bins\n",
    "#     )\n",
    "    \n",
    "#     sample_weights = compute_sample_weights(y_train)\n",
    "    \n",
    "#     print(\"Treinando modelo...\")\n",
    "#     pipeline = create_ensemble_pipeline()\n",
    "#     grid_search = GridSearchCV(\n",
    "#         pipeline,\n",
    "#         param_grid,\n",
    "#         cv=3,\n",
    "#         scoring='r2',\n",
    "#         n_jobs=-1,\n",
    "#         verbose=1\n",
    "#     )\n",
    "    \n",
    "#     warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "#     grid_search.fit(X_train, y_train, ensemble__sample_weight=sample_weights)\n",
    "    \n",
    "#     return grid_search, X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_ensemble_model(X, y, df_original):\n",
    "    \"\"\"Treina o modelo ensemble com ajuste fino de hiperparâmetros\"\"\"\n",
    "    print(\"Preparando features...\")\n",
    "    X_processed = create_features(X, df_original)\n",
    "    X_clean, y_clean = remove_outliers(X_processed, y, z_threshold=2.5)\n",
    "    \n",
    "    y_log = np.log1p(y_clean)\n",
    "    print(\"Criando estratificação...\")\n",
    "    y_bins = pd.qcut(y_log, q=10, labels=False)\n",
    "    \n",
    "    print(\"Dividindo dados...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_clean, y_log,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_bins\n",
    "    )\n",
    "    \n",
    "    sample_weights = compute_sample_weights(y_train)\n",
    "    \n",
    "    # Primeira etapa - Grid inicial\n",
    "    print(\"Treinando modelo - Etapa 1 (Grid Inicial)...\")\n",
    "    pipeline = create_ensemble_pipeline()\n",
    "    grid_search_1 = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid_inicial,\n",
    "        cv=3,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search_1.fit(X_train, y_train, ensemble__sample_weight=sample_weights)\n",
    "    \n",
    "    # Extrair melhores parâmetros\n",
    "    best_params = grid_search_1.best_params_\n",
    "    best_depth = best_params['ensemble__rf__max_depth']\n",
    "    best_split = best_params['ensemble__rf__min_samples_split']\n",
    "    best_lr = best_params['ensemble__gb__learning_rate']\n",
    "    best_subsample = best_params['ensemble__gb__subsample']\n",
    "    \n",
    "    print(f\"\\nMelhores parâmetros da primeira etapa:\")\n",
    "    print(f\"RF max_depth: {best_depth}\")\n",
    "    print(f\"RF min_samples_split: {best_split}\")\n",
    "    print(f\"GB learning_rate: {best_lr}\")\n",
    "    print(f\"GB subsample: {best_subsample}\")\n",
    "    \n",
    "    # Segunda etapa - Grid fino\n",
    "    print(\"\\nTreinando modelo - Etapa 2 (Grid Fino)...\")\n",
    "    param_grid_fino = {\n",
    "        'ensemble__rf__max_depth': [max(4, best_depth-1), best_depth, best_depth+1],\n",
    "        'ensemble__rf__min_samples_split': [max(2, best_split-1), best_split, min(20, best_split+1)],\n",
    "        'ensemble__gb__learning_rate': [best_lr*0.5, best_lr, best_lr*1.5],\n",
    "        'ensemble__gb__subsample': [max(0.5, best_subsample-0.1), best_subsample, min(1.0, best_subsample+0.1)]\n",
    "    }\n",
    "    \n",
    "    grid_search_2 = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid_fino,\n",
    "        cv=3,\n",
    "        scoring=['r2', 'neg_mean_absolute_percentage_error'],\n",
    "        refit='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "    grid_search_2.fit(X_train, y_train, ensemble__sample_weight=sample_weights)\n",
    "    \n",
    "    print(\"\\nMelhores parâmetros finais:\")\n",
    "    print(grid_search_2.best_params_)\n",
    "    \n",
    "    return grid_search_2, X_train, X_test, y_train, y_test\n",
    "\n",
    "# 7. Avaliação\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Avalia o modelo com métricas por faixa de KM\"\"\"\n",
    "    def evaluate_by_km_range(X, y_true, y_pred, km_ranges):\n",
    "        metrics_by_range = {}\n",
    "        for i, (km_min, km_max) in enumerate(km_ranges):\n",
    "            mask = (X['KM Ajustado'] >= km_min) & (X['KM Ajustado'] < km_max)\n",
    "            if mask.any():\n",
    "                metrics_by_range[f'Range {i+1} ({km_min}-{km_max} km)'] = {\n",
    "                    'r2': r2_score(y_true[mask], y_pred[mask]),\n",
    "                    'mape': mean_absolute_percentage_error(y_true[mask], y_pred[mask]),\n",
    "                    'rmse': np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
    "                }\n",
    "        return metrics_by_range\n",
    "    \n",
    "    # Predições e transformações\n",
    "    y_pred_train = np.expm1(model.predict(X_train))\n",
    "    y_pred_test = np.expm1(model.predict(X_test))\n",
    "    \n",
    "    y_train_original = np.expm1(y_train)\n",
    "    y_test_original = np.expm1(y_test)\n",
    "    \n",
    "    # Pós-processamento por faixa\n",
    "    km_ranges = [(0, 100), (100, 300), (300, 600), (600, 1000), (1000, float('inf'))]\n",
    "    \n",
    "    # Métricas gerais\n",
    "    metrics = {\n",
    "        'train_r2': r2_score(y_train_original, y_pred_train),\n",
    "        'test_r2': r2_score(y_test_original, y_pred_test),\n",
    "        'train_mape': mean_absolute_percentage_error(y_train_original, y_pred_train),\n",
    "        'test_mape': mean_absolute_percentage_error(y_test_original, y_pred_test),\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train_original, y_pred_train)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test_original, y_pred_test)),\n",
    "        'train_by_range': evaluate_by_km_range(X_train, y_train_original, y_pred_train, km_ranges),\n",
    "        'test_by_range': evaluate_by_km_range(X_test, y_test_original, y_pred_test, km_ranges)\n",
    "    }\n",
    "    \n",
    "    # Visualizações\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Plot 1: Dispersão\n",
    "    # ax1 = plt.subplot(221)\n",
    "    # ax1.hexbin(y_train_original, y_pred_train, gridsize=30, cmap='YlOrRd')\n",
    "    # ax1.plot([y_train_original.min(), y_train_original.max()], \n",
    "    #          [y_train_original.min(), y_train_original.max()], 'r--')\n",
    "    # ax1.set_title(f'Treino (R² = {metrics[\"train_r2\"]:.3f}, MAPE = {metrics[\"train_mape\"]*100:.1f}%)')\n",
    "\n",
    "    ax1 = plt.subplot(221)\n",
    "    ax1.hexbin(y_test_original, y_pred_test, gridsize=30, cmap='YlOrRd')\n",
    "    ax1.plot([y_test_original.min(), y_test_original.max()], \n",
    "             [y_test_original.min(), y_test_original.max()], 'r--')\n",
    "    ax1.set_title(f'Teste (R² = {metrics[\"test_r2\"]:.2f}, MAPE = {metrics[\"test_mape\"]*100:.1f}%)')\n",
    "    ax1.set_xlabel('Valor Real')\n",
    "    ax1.set_ylabel('Valor Previsto')\n",
    "    \n",
    "    # Plot 2: Residuais\n",
    "    ax2 = plt.subplot(222)\n",
    "    residuals = y_test_original - y_pred_test\n",
    "    ax2.scatter(y_pred_test, residuals, alpha=0.5)\n",
    "    ax2.axhline(y=0, color='r', linestyle='--')\n",
    "    ax2.set_title('Análise de Residuais (Teste)')\n",
    "    \n",
    "    # Plot 3: Distribuição dos Erros\n",
    "    ax3 = plt.subplot(223)\n",
    "    ax3.hist(residuals, bins=50)\n",
    "    ax3.set_title('Distribuição dos Erros')\n",
    "    \n",
    "    # Plot 4: Erro por Faixa de KM\n",
    "    ax4 = plt.subplot(224)\n",
    "    mape_by_range = []\n",
    "    range_labels = []\n",
    "    for range_name, metrics_range in metrics['test_by_range'].items():\n",
    "        mape_by_range.append(metrics_range['mape'] * 100)\n",
    "        range_labels.append(range_name)\n",
    "    ax4.bar(range_labels, mape_by_range)\n",
    "    ax4.set_title('MAPE por Faixa de KM')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return metrics, fig\n",
    "\n",
    "# 8. Execução\n",
    "# print(\"Iniciando treinamento...\")\n",
    "# model, X_train, X_test, y_train, y_test = train_ensemble_model(X_Model, y_target, df_filt)\n",
    "# metrics, fig = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "print(\"Iniciando treinamento com ajuste fino...\")\n",
    "model, X_train, X_test, y_train, y_test = train_ensemble_model(X_Model, y_target, df_filt)\n",
    "metrics, fig = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 9. Salvamento e resultados\n",
    "os.makedirs(path_out_images, exist_ok=True)\n",
    "os.makedirs(path_out, exist_ok=True)\n",
    "fig.savefig(os.path.join(path_out_images, 'Ensemble_Model_Evaluation_v2.png'), dpi=200)\n",
    "pickle.dump(model, open(os.path.join(path_out, \"pepsico_frete_ensemble_v2.sav\"), 'wb'))\n",
    "\n",
    "print(\"\\nMétricas de Avaliação:\")\n",
    "print(f\"R² Treino: {metrics['train_r2']:.3f}\")\n",
    "print(f\"R² Teste: {metrics['test_r2']:.3f}\")\n",
    "print(f\"MAPE Treino: {metrics['train_mape']*100:.1f}%\")\n",
    "print(f\"MAPE Teste: {metrics['test_mape']*100:.1f}%\")\n",
    "print(f\"RMSE Treino: {metrics['train_rmse']:.2f}\")\n",
    "print(f\"RMSE Teste: {metrics['test_rmse']:.2f}\")\n",
    "\n",
    "# 10. Salvar resultados\n",
    "os.makedirs(path_out_images, exist_ok=True)\n",
    "os.makedirs(path_out, exist_ok=True)\n",
    "fig.savefig(os.path.join(path_out_images, 'Ensemble_Model_Evaluation.png'), dpi=200)\n",
    "pickle.dump(model, open(os.path.join(path_out, \"pepsico_frete_ensemble.sav\"), 'wb'))\n",
    "\n",
    "# 11. Imprimir resultados detalhados\n",
    "print(\"\\nMétricas de Avaliação:\")\n",
    "print(f\"R² Treino: {metrics['train_r2']:.3f}\")\n",
    "print(f\"R² Teste: {metrics['test_r2']:.3f}\")\n",
    "print(f\"MAPE Treino: {metrics['train_mape']*100:.1f}%\")\n",
    "print(f\"MAPE Teste: {metrics['test_mape']*100:.1f}%\")\n",
    "print(f\"RMSE Treino: {metrics['train_rmse']:.2f}\")\n",
    "print(f\"RMSE Teste: {metrics['test_rmse']:.2f}\")\n",
    "\n",
    "# 12. Análise de importância das features\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "})\n",
    "\n",
    "print(\"\\nImportância das Features (top 10):\")\n",
    "print(feature_importance.sort_values('importance', ascending=False).head(10))\n",
    "\n",
    "# 13. Plot da importância das features\n",
    "plt.figure(figsize=(12, 6))\n",
    "feature_importance.sort_values('importance').tail(10).plot(\n",
    "    x='feature', \n",
    "    y='importance', \n",
    "    kind='barh'\n",
    ")\n",
    "plt.title('Top 10 Features Mais Importantes')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(path_out_images, 'Feature_Importance.png'), dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Planta_Class', 'Movimentação_Class', 'UF Destino_Class', 'TIPO_Class']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
